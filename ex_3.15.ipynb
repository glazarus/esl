{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the spam training data, split in to a training and test set\n",
    "with open('spam/spam.data', 'r') as f:\n",
    "    all_data = np.array([\n",
    "        [float(value) for value in line.split()]\n",
    "        for line in f\n",
    "    ])\n",
    "\n",
    "all_train, all_test = train_test_split(all_data, test_size=.2)\n",
    "\n",
    "# set all data to mean 0, std 1 ( needed by some of the regression methods )\n",
    "scaler = StandardScaler().fit(all_train)\n",
    "all_train = scaler.transform(all_train)\n",
    "all_test = scaler.transform(all_test)\n",
    "\n",
    "loaded_train_X, loaded_train_Y = all_train[:, :-1], all_train[:, -1]\n",
    "loaded_test_X, loaded_test_Y = all_test[:, :-1], all_test[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_table(train_X, train_Y, test_X, test_Y):\n",
    "    # train each of our models on the training set\n",
    "    ols_model = LinearRegression(fit_intercept=False).fit(train_X, train_Y)\n",
    "\n",
    "    lasso_model = LassoCV(cv=8, fit_intercept=False).fit(train_X, train_Y)\n",
    "\n",
    "    ridge_model = RidgeCV(cv=8, fit_intercept=False).fit(train_X, train_Y)\n",
    "\n",
    "    feature_select = RFECV(LinearRegression(fit_intercept=False), cv=8).fit(train_X, train_Y)\n",
    "\n",
    "    class FeatureSelectWrapper(object):\n",
    "        \"\"\"\n",
    "        Wraps RFECV to produce a coefficent vector with 0s at ununsed columns rather than a smaller vector\n",
    "        \"\"\"\n",
    "        def __init__(self, real_feature_select):\n",
    "            self.coef_ = []\n",
    "            self._estimator = real_feature_select\n",
    "            coef_index = 0\n",
    "            for mask in real_feature_select.support_:\n",
    "                if mask:\n",
    "                    self.coef_.append(real_feature_select.estimator_.coef_[coef_index])\n",
    "                else:\n",
    "                    self.coef_.append(0)\n",
    "\n",
    "        def predict(self, x_values):\n",
    "            return self._estimator.predict(x_values)\n",
    "\n",
    "    feature_select_wrapper = FeatureSelectWrapper(feature_select)\n",
    "\n",
    "    # sklearn doesn't implement Principle Component Regression, so implement it ourselves\n",
    "    def pcr(X, Y, k):\n",
    "        assert k <= X.shape[1]\n",
    "        u, s, v = np.linalg.svd(X)\n",
    "        vk = v[:, :k]\n",
    "        Wk = np.dot(X, vk)\n",
    "        gamma_hat = np.dot(np.dot(np.linalg.inv(np.dot(Wk.T, Wk)), Wk.T), Y)\n",
    "        return np.dot(vk, gamma_hat)\n",
    "\n",
    "    class PCREstimator(BaseEstimator):\n",
    "        def __init__(self, directions=1):\n",
    "            self.coef_ = None\n",
    "            self.directions=directions\n",
    "\n",
    "        def fit(self, X, y):\n",
    "            self.coef_ = pcr(X, y, self.directions)\n",
    "\n",
    "        def predict(self, X):\n",
    "            assert self.coef_ is not None, 'Estimator not fit'\n",
    "            return np.dot(X, self.coef_)\n",
    "\n",
    "    fit_pcr = GridSearchCV(PCREstimator(), {'directions': range(1, train_X.shape[1] + 1)}, 'r2', cv=8).fit(train_X, train_Y)\n",
    "\n",
    "    fit_pls = GridSearchCV(PLSRegression(), {'n_components': range(1, train_X.shape[1] + 1)}, 'r2', cv=8, return_train_score=True).fit(train_X, train_Y)\n",
    "\n",
    "    # calculate column coefficents and train/test error for each model\n",
    "    all_estimators = [\n",
    "        ('LS', ols_model),\n",
    "        ('Best Subset', feature_select_wrapper),\n",
    "        ('Ridge', ridge_model),\n",
    "        ('Lasso', lasso_model),\n",
    "        ('PCR', fit_pcr.best_estimator_),\n",
    "        ('PLS', fit_pls.best_estimator_)\n",
    "    ]\n",
    "\n",
    "    all_coefs = np.array([\n",
    "        np.reshape(estimator.coef_, test_X.shape[1])\n",
    "        for _, estimator in all_estimators\n",
    "    ]).T\n",
    "\n",
    "    # set all 0 values to n/a so it appears as blank in the table\n",
    "    zero_value_mask = abs(all_coefs) < .000001\n",
    "    all_coefs[zero_value_mask] = np.nan\n",
    "\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    for _, estimator in all_estimators:\n",
    "        train_errors.append(mean_squared_error(train_Y, estimator.predict(train_X)))\n",
    "        test_errors.append(mean_squared_error(test_Y, estimator.predict(test_X)))\n",
    "\n",
    "    train_test_errors = pd.DataFrame(\n",
    "        [train_errors, test_errors], \n",
    "        columns=[name for name, _ in all_estimators],\n",
    "        index=['Train Error', 'Test Error'])\n",
    "\n",
    "    # display the results\n",
    "    return pd.DataFrame(\n",
    "        all_coefs, \n",
    "        columns=[name for name, _ in all_estimators]).fillna('').append(train_test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/scipy/linalg/basic.py:1226: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/cross_decomposition/pls_.py:310: RuntimeWarning: invalid value encountered in divide\n",
      "  y_scores = np.dot(Yk, y_weights) / y_ss\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LS</th>\n",
       "      <th>Best Subset</th>\n",
       "      <th>Ridge</th>\n",
       "      <th>Lasso</th>\n",
       "      <th>PCR</th>\n",
       "      <th>PLS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.041823</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>-0.041539</td>\n",
       "      <td>-0.0266498</td>\n",
       "      <td>-0.032742</td>\n",
       "      <td>-0.035798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.031859</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>-0.031755</td>\n",
       "      <td>-0.0195385</td>\n",
       "      <td>-0.023520</td>\n",
       "      <td>-0.024274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.033843</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.033857</td>\n",
       "      <td>0.0293969</td>\n",
       "      <td>0.041320</td>\n",
       "      <td>0.034527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.036836</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.036787</td>\n",
       "      <td>0.0300241</td>\n",
       "      <td>0.058269</td>\n",
       "      <td>0.042986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.115191</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.114969</td>\n",
       "      <td>0.111421</td>\n",
       "      <td>0.106953</td>\n",
       "      <td>0.113864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.068272</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.068251</td>\n",
       "      <td>0.0667026</td>\n",
       "      <td>0.028559</td>\n",
       "      <td>0.074468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.172547</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.172218</td>\n",
       "      <td>0.17416</td>\n",
       "      <td>0.155890</td>\n",
       "      <td>0.174775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.074854</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.074747</td>\n",
       "      <td>0.0721209</td>\n",
       "      <td>0.082752</td>\n",
       "      <td>0.075557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.045650</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.045627</td>\n",
       "      <td>0.0444633</td>\n",
       "      <td>0.073553</td>\n",
       "      <td>0.038958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.015424</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.015456</td>\n",
       "      <td>0.0104842</td>\n",
       "      <td>0.014479</td>\n",
       "      <td>0.013577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.028264</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.028351</td>\n",
       "      <td>0.0237205</td>\n",
       "      <td>0.056396</td>\n",
       "      <td>0.034001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.042263</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>-0.042133</td>\n",
       "      <td>-0.0325767</td>\n",
       "      <td>-0.032952</td>\n",
       "      <td>-0.046171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.011211</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.011298</td>\n",
       "      <td>0.00877653</td>\n",
       "      <td>0.048684</td>\n",
       "      <td>0.016105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.008551</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.008594</td>\n",
       "      <td>0.00158504</td>\n",
       "      <td>0.044407</td>\n",
       "      <td>0.008559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.015094</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.015226</td>\n",
       "      <td>0.0138037</td>\n",
       "      <td>0.020805</td>\n",
       "      <td>0.021546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.109964</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.109812</td>\n",
       "      <td>0.107374</td>\n",
       "      <td>0.078617</td>\n",
       "      <td>0.121001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.053030</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.053103</td>\n",
       "      <td>0.0513693</td>\n",
       "      <td>0.071979</td>\n",
       "      <td>0.070133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.050711</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.050673</td>\n",
       "      <td>0.0446756</td>\n",
       "      <td>0.080665</td>\n",
       "      <td>0.051920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.046554</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.046658</td>\n",
       "      <td>0.0477933</td>\n",
       "      <td>0.021297</td>\n",
       "      <td>0.062277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.057261</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.057231</td>\n",
       "      <td>0.0563257</td>\n",
       "      <td>0.069172</td>\n",
       "      <td>0.061592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.115852</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.115681</td>\n",
       "      <td>0.1188</td>\n",
       "      <td>0.045539</td>\n",
       "      <td>0.109356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.097083</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.096733</td>\n",
       "      <td>0.0877439</td>\n",
       "      <td>0.127320</td>\n",
       "      <td>0.082723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.119370</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.119164</td>\n",
       "      <td>0.119974</td>\n",
       "      <td>0.137183</td>\n",
       "      <td>0.110918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.096109</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.095900</td>\n",
       "      <td>0.0911153</td>\n",
       "      <td>0.073276</td>\n",
       "      <td>0.084002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.080191</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>-0.080052</td>\n",
       "      <td>-0.0767566</td>\n",
       "      <td>-0.026443</td>\n",
       "      <td>-0.074818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.039451</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>-0.039498</td>\n",
       "      <td>-0.0372472</td>\n",
       "      <td>0.004652</td>\n",
       "      <td>-0.050721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.090098</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>-0.089884</td>\n",
       "      <td>-0.078689</td>\n",
       "      <td>-0.045741</td>\n",
       "      <td>-0.076087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.004162</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.004019</td>\n",
       "      <td></td>\n",
       "      <td>-0.040892</td>\n",
       "      <td>-0.008823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.007925</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>-0.007999</td>\n",
       "      <td></td>\n",
       "      <td>-0.058299</td>\n",
       "      <td>-0.015420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.057881</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>-0.057604</td>\n",
       "      <td>-0.0334188</td>\n",
       "      <td>-0.124592</td>\n",
       "      <td>-0.035511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.013181</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>-0.013104</td>\n",
       "      <td></td>\n",
       "      <td>-0.072975</td>\n",
       "      <td>-0.003134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.004953</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.012773</td>\n",
       "      <td></td>\n",
       "      <td>0.097404</td>\n",
       "      <td>0.018243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.058666</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>-0.058544</td>\n",
       "      <td>-0.0522676</td>\n",
       "      <td>-0.081541</td>\n",
       "      <td>-0.053627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.044071</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.036030</td>\n",
       "      <td>0.0127209</td>\n",
       "      <td>0.055692</td>\n",
       "      <td>0.018447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.032655</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>-0.032536</td>\n",
       "      <td>-0.0180113</td>\n",
       "      <td>-0.001961</td>\n",
       "      <td>-0.021488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.013425</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.013258</td>\n",
       "      <td></td>\n",
       "      <td>0.007785</td>\n",
       "      <td>0.004216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.025523</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>-0.025563</td>\n",
       "      <td>-0.026499</td>\n",
       "      <td>-0.063015</td>\n",
       "      <td>-0.032117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.024948</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>-0.024880</td>\n",
       "      <td>-0.0153739</td>\n",
       "      <td>-0.037071</td>\n",
       "      <td>-0.020956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.022018</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>-0.022068</td>\n",
       "      <td>-0.0191106</td>\n",
       "      <td>-0.018258</td>\n",
       "      <td>-0.029856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.020631</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.020611</td>\n",
       "      <td>0.0120578</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.025708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.012648</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>-0.012725</td>\n",
       "      <td>-0.00932291</td>\n",
       "      <td>-0.006734</td>\n",
       "      <td>-0.021721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.061006</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>-0.060863</td>\n",
       "      <td>-0.0605327</td>\n",
       "      <td>-0.037488</td>\n",
       "      <td>-0.058433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.029780</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>-0.029770</td>\n",
       "      <td>-0.0241806</td>\n",
       "      <td>-0.029270</td>\n",
       "      <td>-0.030627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.043106</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>-0.043027</td>\n",
       "      <td>-0.0357287</td>\n",
       "      <td>-0.044580</td>\n",
       "      <td>-0.041070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.069720</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>-0.069582</td>\n",
       "      <td>-0.0618911</td>\n",
       "      <td>-0.067280</td>\n",
       "      <td>-0.063527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.070689</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>-0.070533</td>\n",
       "      <td>-0.0632739</td>\n",
       "      <td>-0.060019</td>\n",
       "      <td>-0.063168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.033497</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>-0.033422</td>\n",
       "      <td>-0.0244463</td>\n",
       "      <td>-0.025951</td>\n",
       "      <td>-0.035279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.033843</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>-0.033788</td>\n",
       "      <td>-0.0263861</td>\n",
       "      <td>-0.044815</td>\n",
       "      <td>-0.034177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-0.070667</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>-0.070337</td>\n",
       "      <td>-0.0580812</td>\n",
       "      <td>-0.133935</td>\n",
       "      <td>-0.038962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-0.027891</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>-0.027838</td>\n",
       "      <td>-0.0117488</td>\n",
       "      <td>0.007346</td>\n",
       "      <td>-0.029466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-0.020532</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>-0.020484</td>\n",
       "      <td>-0.0131368</td>\n",
       "      <td>-0.004348</td>\n",
       "      <td>-0.015943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.111059</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.110858</td>\n",
       "      <td>0.107424</td>\n",
       "      <td>0.105346</td>\n",
       "      <td>0.107703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.115221</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.115101</td>\n",
       "      <td>0.114434</td>\n",
       "      <td>0.107717</td>\n",
       "      <td>0.114685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.023731</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.023719</td>\n",
       "      <td>0.0149127</td>\n",
       "      <td>0.032195</td>\n",
       "      <td>0.031581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.017516</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.017623</td>\n",
       "      <td>0.0156665</td>\n",
       "      <td>-0.039213</td>\n",
       "      <td>0.014002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.021103</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.021193</td>\n",
       "      <td>0.011447</td>\n",
       "      <td>0.032783</td>\n",
       "      <td>0.023262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.099678</td>\n",
       "      <td>-0.041823</td>\n",
       "      <td>0.099444</td>\n",
       "      <td>0.101572</td>\n",
       "      <td>0.081783</td>\n",
       "      <td>0.075124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Error</th>\n",
       "      <td>0.441987</td>\n",
       "      <td>0.441987</td>\n",
       "      <td>0.441989</td>\n",
       "      <td>0.444922</td>\n",
       "      <td>0.477485</td>\n",
       "      <td>0.445602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Error</th>\n",
       "      <td>0.453544</td>\n",
       "      <td>0.453544</td>\n",
       "      <td>0.453497</td>\n",
       "      <td>0.452998</td>\n",
       "      <td>0.505890</td>\n",
       "      <td>0.453357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   LS  Best Subset     Ridge       Lasso       PCR       PLS\n",
       "0           -0.041823    -0.041823 -0.041539  -0.0266498 -0.032742 -0.035798\n",
       "1           -0.031859    -0.041823 -0.031755  -0.0195385 -0.023520 -0.024274\n",
       "2            0.033843    -0.041823  0.033857   0.0293969  0.041320  0.034527\n",
       "3            0.036836    -0.041823  0.036787   0.0300241  0.058269  0.042986\n",
       "4            0.115191    -0.041823  0.114969    0.111421  0.106953  0.113864\n",
       "5            0.068272    -0.041823  0.068251   0.0667026  0.028559  0.074468\n",
       "6            0.172547    -0.041823  0.172218     0.17416  0.155890  0.174775\n",
       "7            0.074854    -0.041823  0.074747   0.0721209  0.082752  0.075557\n",
       "8            0.045650    -0.041823  0.045627   0.0444633  0.073553  0.038958\n",
       "9            0.015424    -0.041823  0.015456   0.0104842  0.014479  0.013577\n",
       "10           0.028264    -0.041823  0.028351   0.0237205  0.056396  0.034001\n",
       "11          -0.042263    -0.041823 -0.042133  -0.0325767 -0.032952 -0.046171\n",
       "12           0.011211    -0.041823  0.011298  0.00877653  0.048684  0.016105\n",
       "13           0.008551    -0.041823  0.008594  0.00158504  0.044407  0.008559\n",
       "14           0.015094    -0.041823  0.015226   0.0138037  0.020805  0.021546\n",
       "15           0.109964    -0.041823  0.109812    0.107374  0.078617  0.121001\n",
       "16           0.053030    -0.041823  0.053103   0.0513693  0.071979  0.070133\n",
       "17           0.050711    -0.041823  0.050673   0.0446756  0.080665  0.051920\n",
       "18           0.046554    -0.041823  0.046658   0.0477933  0.021297  0.062277\n",
       "19           0.057261    -0.041823  0.057231   0.0563257  0.069172  0.061592\n",
       "20           0.115852    -0.041823  0.115681      0.1188  0.045539  0.109356\n",
       "21           0.097083    -0.041823  0.096733   0.0877439  0.127320  0.082723\n",
       "22           0.119370    -0.041823  0.119164    0.119974  0.137183  0.110918\n",
       "23           0.096109    -0.041823  0.095900   0.0911153  0.073276  0.084002\n",
       "24          -0.080191    -0.041823 -0.080052  -0.0767566 -0.026443 -0.074818\n",
       "25          -0.039451    -0.041823 -0.039498  -0.0372472  0.004652 -0.050721\n",
       "26          -0.090098    -0.041823 -0.089884   -0.078689 -0.045741 -0.076087\n",
       "27           0.004162    -0.041823  0.004019             -0.040892 -0.008823\n",
       "28          -0.007925    -0.041823 -0.007999             -0.058299 -0.015420\n",
       "29          -0.057881    -0.041823 -0.057604  -0.0334188 -0.124592 -0.035511\n",
       "30          -0.013181    -0.041823 -0.013104             -0.072975 -0.003134\n",
       "31           0.004953    -0.041823  0.012773              0.097404  0.018243\n",
       "32          -0.058666    -0.041823 -0.058544  -0.0522676 -0.081541 -0.053627\n",
       "33           0.044071    -0.041823  0.036030   0.0127209  0.055692  0.018447\n",
       "34          -0.032655    -0.041823 -0.032536  -0.0180113 -0.001961 -0.021488\n",
       "35           0.013425    -0.041823  0.013258              0.007785  0.004216\n",
       "36          -0.025523    -0.041823 -0.025563   -0.026499 -0.063015 -0.032117\n",
       "37          -0.024948    -0.041823 -0.024880  -0.0153739 -0.037071 -0.020956\n",
       "38          -0.022018    -0.041823 -0.022068  -0.0191106 -0.018258 -0.029856\n",
       "39           0.020631    -0.041823  0.020611   0.0120578  0.000347  0.025708\n",
       "40          -0.012648    -0.041823 -0.012725 -0.00932291 -0.006734 -0.021721\n",
       "41          -0.061006    -0.041823 -0.060863  -0.0605327 -0.037488 -0.058433\n",
       "42          -0.029780    -0.041823 -0.029770  -0.0241806 -0.029270 -0.030627\n",
       "43          -0.043106    -0.041823 -0.043027  -0.0357287 -0.044580 -0.041070\n",
       "44          -0.069720    -0.041823 -0.069582  -0.0618911 -0.067280 -0.063527\n",
       "45          -0.070689    -0.041823 -0.070533  -0.0632739 -0.060019 -0.063168\n",
       "46          -0.033497    -0.041823 -0.033422  -0.0244463 -0.025951 -0.035279\n",
       "47          -0.033843    -0.041823 -0.033788  -0.0263861 -0.044815 -0.034177\n",
       "48          -0.070667    -0.041823 -0.070337  -0.0580812 -0.133935 -0.038962\n",
       "49          -0.027891    -0.041823 -0.027838  -0.0117488  0.007346 -0.029466\n",
       "50          -0.020532    -0.041823 -0.020484  -0.0131368 -0.004348 -0.015943\n",
       "51           0.111059    -0.041823  0.110858    0.107424  0.105346  0.107703\n",
       "52           0.115221    -0.041823  0.115101    0.114434  0.107717  0.114685\n",
       "53           0.023731    -0.041823  0.023719   0.0149127  0.032195  0.031581\n",
       "54           0.017516    -0.041823  0.017623   0.0156665 -0.039213  0.014002\n",
       "55           0.021103    -0.041823  0.021193    0.011447  0.032783  0.023262\n",
       "56           0.099678    -0.041823  0.099444    0.101572  0.081783  0.075124\n",
       "Train Error  0.441987     0.441987  0.441989    0.444922  0.477485  0.445602\n",
       "Test Error   0.453544     0.453544  0.453497    0.452998  0.505890  0.453357"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a table matching books for spam data\n",
    "display_table(loaded_train_X, loaded_train_Y, loaded_test_X, loaded_test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
