{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the spam training data, split in to a training and test set\n",
    "with open('spam/spam.data', 'r') as f:\n",
    "    all_data = np.array([\n",
    "        [float(value) for value in line.split()]\n",
    "        for line in f\n",
    "    ])\n",
    "\n",
    "all_train, all_test = train_test_split(all_data, test_size=.2)\n",
    "\n",
    "# set all data to mean 0, std 1 ( needed by some of the regression methods )\n",
    "scaler = StandardScaler().fit(all_train)\n",
    "all_train = scaler.transform(all_train)\n",
    "all_test = scaler.transform(all_test)\n",
    "\n",
    "loaded_train_X, loaded_train_Y = all_train[:, :-1], all_train[:, -1]\n",
    "loaded_test_X, loaded_test_Y = all_test[:, :-1], all_test[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_table(train_X, train_Y, test_X, test_Y):\n",
    "    # train each of our models on the training set\n",
    "    ols_model = LinearRegression(fit_intercept=False).fit(train_X, train_Y)\n",
    "\n",
    "    lasso_model = LassoCV(cv=8, fit_intercept=False).fit(train_X, train_Y)\n",
    "\n",
    "    ridge_model = RidgeCV(cv=8, fit_intercept=False).fit(train_X, train_Y)\n",
    "\n",
    "    feature_select = RFECV(LinearRegression(fit_intercept=False), cv=8).fit(train_X, train_Y)\n",
    "\n",
    "    class FeatureSelectWrapper(object):\n",
    "        \"\"\"\n",
    "        Wraps RFECV to produce a coefficent vector with 0s at ununsed columns rather than a smaller vector\n",
    "        \"\"\"\n",
    "        def __init__(self, real_feature_select):\n",
    "            self.coef_ = []\n",
    "            self._estimator = real_feature_select\n",
    "            coef_index = 0\n",
    "            for mask in real_feature_select.support_:\n",
    "                if mask:\n",
    "                    self.coef_.append(real_feature_select.estimator_.coef_[coef_index])\n",
    "                    coef_index += 1\n",
    "                else:\n",
    "                    self.coef_.append(0)\n",
    "\n",
    "        def predict(self, x_values):\n",
    "            return self._estimator.predict(x_values)\n",
    "\n",
    "    feature_select_wrapper = FeatureSelectWrapper(feature_select)\n",
    "\n",
    "    # sklearn doesn't implement Principle Component Regression, so implement it ourselves\n",
    "    def pcr(X, Y, k):\n",
    "        assert k <= X.shape[1]\n",
    "        u, s, v = np.linalg.svd(X)\n",
    "        vk = v[:, :k]\n",
    "        Wk = np.dot(X, vk)\n",
    "        gamma_hat = np.dot(np.dot(np.linalg.inv(np.dot(Wk.T, Wk)), Wk.T), Y)\n",
    "        return np.dot(vk, gamma_hat)\n",
    "\n",
    "    class PCREstimator(BaseEstimator):\n",
    "        def __init__(self, directions=1):\n",
    "            self.coef_ = None\n",
    "            self.directions=directions\n",
    "\n",
    "        def fit(self, X, y):\n",
    "            self.coef_ = pcr(X, y, self.directions)\n",
    "\n",
    "        def predict(self, X):\n",
    "            assert self.coef_ is not None, 'Estimator not fit'\n",
    "            return np.dot(X, self.coef_)\n",
    "\n",
    "    fit_pcr = GridSearchCV(PCREstimator(), {'directions': range(1, train_X.shape[1] + 1)}, 'r2', cv=8).fit(train_X, train_Y)\n",
    "\n",
    "    fit_pls = GridSearchCV(PLSRegression(), {'n_components': range(1, train_X.shape[1] + 1)}, 'r2', cv=8, return_train_score=True).fit(train_X, train_Y)\n",
    "\n",
    "    # calculate column coefficents and train/test error for each model\n",
    "    all_estimators = [\n",
    "        ('LS', ols_model),\n",
    "        ('Best Subset', feature_select_wrapper),\n",
    "        ('Ridge', ridge_model),\n",
    "        ('Lasso', lasso_model),\n",
    "        ('PCR', fit_pcr.best_estimator_),\n",
    "        ('PLS', fit_pls.best_estimator_)\n",
    "    ]\n",
    "\n",
    "    all_coefs = np.array([\n",
    "        np.reshape(estimator.coef_, test_X.shape[1])\n",
    "        for _, estimator in all_estimators\n",
    "    ]).T\n",
    "\n",
    "    # set all 0 values to n/a so it appears as blank in the table\n",
    "    zero_value_mask = abs(all_coefs) < .000001\n",
    "    all_coefs[zero_value_mask] = np.nan\n",
    "\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    for _, estimator in all_estimators:\n",
    "        train_errors.append(mean_squared_error(train_Y, estimator.predict(train_X)))\n",
    "        test_errors.append(mean_squared_error(test_Y, estimator.predict(test_X)))\n",
    "\n",
    "    train_test_errors = pd.DataFrame(\n",
    "        [train_errors, test_errors], \n",
    "        columns=[name for name, _ in all_estimators],\n",
    "        index=['Train Error', 'Test Error'])\n",
    "\n",
    "    # display the results\n",
    "    return pd.DataFrame(\n",
    "        all_coefs, \n",
    "        columns=[name for name, _ in all_estimators]).fillna('').append(train_test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/scipy/linalg/basic.py:1226: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LS</th>\n",
       "      <th>Best Subset</th>\n",
       "      <th>Ridge</th>\n",
       "      <th>Lasso</th>\n",
       "      <th>PCR</th>\n",
       "      <th>PLS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.036110</td>\n",
       "      <td>-0.036637</td>\n",
       "      <td>-0.035866</td>\n",
       "      <td>-0.0267818</td>\n",
       "      <td>-0.033798</td>\n",
       "      <td>-0.036663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.030507</td>\n",
       "      <td>-0.0298764</td>\n",
       "      <td>-0.030408</td>\n",
       "      <td>-0.0234702</td>\n",
       "      <td>-0.046519</td>\n",
       "      <td>-0.030949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.035183</td>\n",
       "      <td>0.0360065</td>\n",
       "      <td>0.035209</td>\n",
       "      <td>0.0323751</td>\n",
       "      <td>0.071214</td>\n",
       "      <td>0.035090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.038179</td>\n",
       "      <td>0.0380054</td>\n",
       "      <td>0.038124</td>\n",
       "      <td>0.0341091</td>\n",
       "      <td>0.028679</td>\n",
       "      <td>0.038801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.110689</td>\n",
       "      <td>0.111304</td>\n",
       "      <td>0.110466</td>\n",
       "      <td>0.107658</td>\n",
       "      <td>0.106853</td>\n",
       "      <td>0.110986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.060405</td>\n",
       "      <td>0.0606352</td>\n",
       "      <td>0.060368</td>\n",
       "      <td>0.058904</td>\n",
       "      <td>0.061114</td>\n",
       "      <td>0.060327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.175143</td>\n",
       "      <td>0.175378</td>\n",
       "      <td>0.174792</td>\n",
       "      <td>0.175833</td>\n",
       "      <td>0.163806</td>\n",
       "      <td>0.175843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.081245</td>\n",
       "      <td>0.0819069</td>\n",
       "      <td>0.081119</td>\n",
       "      <td>0.0798151</td>\n",
       "      <td>0.087373</td>\n",
       "      <td>0.081557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.059772</td>\n",
       "      <td>0.0623436</td>\n",
       "      <td>0.059723</td>\n",
       "      <td>0.05934</td>\n",
       "      <td>0.062577</td>\n",
       "      <td>0.059179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.011132</td>\n",
       "      <td></td>\n",
       "      <td>0.011173</td>\n",
       "      <td>0.0084412</td>\n",
       "      <td>0.013562</td>\n",
       "      <td>0.009840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.024762</td>\n",
       "      <td>0.0259669</td>\n",
       "      <td>0.024868</td>\n",
       "      <td>0.0221559</td>\n",
       "      <td>0.022915</td>\n",
       "      <td>0.023782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.050242</td>\n",
       "      <td>-0.0506995</td>\n",
       "      <td>-0.050070</td>\n",
       "      <td>-0.0443741</td>\n",
       "      <td>-0.051794</td>\n",
       "      <td>-0.048921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.001425</td>\n",
       "      <td></td>\n",
       "      <td>0.001521</td>\n",
       "      <td></td>\n",
       "      <td>0.003185</td>\n",
       "      <td>0.000387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.001205</td>\n",
       "      <td></td>\n",
       "      <td>0.001259</td>\n",
       "      <td></td>\n",
       "      <td>-0.000210</td>\n",
       "      <td>0.002607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.009176</td>\n",
       "      <td></td>\n",
       "      <td>0.009375</td>\n",
       "      <td>0.00878644</td>\n",
       "      <td>-0.000502</td>\n",
       "      <td>0.012162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.111747</td>\n",
       "      <td>0.11146</td>\n",
       "      <td>0.111604</td>\n",
       "      <td>0.110345</td>\n",
       "      <td>0.114369</td>\n",
       "      <td>0.112386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.040973</td>\n",
       "      <td>0.0411687</td>\n",
       "      <td>0.041118</td>\n",
       "      <td>0.0404225</td>\n",
       "      <td>0.040790</td>\n",
       "      <td>0.040286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.059603</td>\n",
       "      <td>0.0615841</td>\n",
       "      <td>0.059549</td>\n",
       "      <td>0.0568506</td>\n",
       "      <td>0.062264</td>\n",
       "      <td>0.058067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.059005</td>\n",
       "      <td>0.0598366</td>\n",
       "      <td>0.059048</td>\n",
       "      <td>0.0590946</td>\n",
       "      <td>0.054173</td>\n",
       "      <td>0.060147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.063041</td>\n",
       "      <td>0.0626461</td>\n",
       "      <td>0.062978</td>\n",
       "      <td>0.0614178</td>\n",
       "      <td>0.061859</td>\n",
       "      <td>0.063540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.127755</td>\n",
       "      <td>0.127696</td>\n",
       "      <td>0.127561</td>\n",
       "      <td>0.129239</td>\n",
       "      <td>0.126516</td>\n",
       "      <td>0.126854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.099860</td>\n",
       "      <td>0.100121</td>\n",
       "      <td>0.099527</td>\n",
       "      <td>0.0948315</td>\n",
       "      <td>0.100711</td>\n",
       "      <td>0.096833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.123441</td>\n",
       "      <td>0.119767</td>\n",
       "      <td>0.119998</td>\n",
       "      <td>0.127623</td>\n",
       "      <td>0.117878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.077301</td>\n",
       "      <td>0.077647</td>\n",
       "      <td>0.077177</td>\n",
       "      <td>0.0737166</td>\n",
       "      <td>0.072786</td>\n",
       "      <td>0.078084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.077107</td>\n",
       "      <td>-0.077094</td>\n",
       "      <td>-0.076984</td>\n",
       "      <td>-0.0746125</td>\n",
       "      <td>-0.072865</td>\n",
       "      <td>-0.077810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.041225</td>\n",
       "      <td>-0.0410929</td>\n",
       "      <td>-0.041257</td>\n",
       "      <td>-0.0398726</td>\n",
       "      <td>-0.046861</td>\n",
       "      <td>-0.041525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.084050</td>\n",
       "      <td>-0.0839872</td>\n",
       "      <td>-0.083867</td>\n",
       "      <td>-0.0778929</td>\n",
       "      <td>-0.081057</td>\n",
       "      <td>-0.086010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.005401</td>\n",
       "      <td></td>\n",
       "      <td>0.005218</td>\n",
       "      <td></td>\n",
       "      <td>-0.003230</td>\n",
       "      <td>0.001284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.009712</td>\n",
       "      <td></td>\n",
       "      <td>-0.009750</td>\n",
       "      <td>-0.00015639</td>\n",
       "      <td>-0.020460</td>\n",
       "      <td>-0.009257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.045091</td>\n",
       "      <td>-0.044008</td>\n",
       "      <td>-0.044876</td>\n",
       "      <td>-0.0328331</td>\n",
       "      <td>-0.039689</td>\n",
       "      <td>-0.040435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.020500</td>\n",
       "      <td>-0.0210813</td>\n",
       "      <td>-0.020315</td>\n",
       "      <td>-0.00295302</td>\n",
       "      <td>-0.016801</td>\n",
       "      <td>-0.014108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.026319</td>\n",
       "      <td></td>\n",
       "      <td>0.023337</td>\n",
       "      <td>0.00746307</td>\n",
       "      <td>-0.057982</td>\n",
       "      <td>0.017925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.042810</td>\n",
       "      <td>-0.0452533</td>\n",
       "      <td>-0.042741</td>\n",
       "      <td>-0.0395651</td>\n",
       "      <td>-0.039637</td>\n",
       "      <td>-0.043188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.011649</td>\n",
       "      <td>0.0350203</td>\n",
       "      <td>0.014424</td>\n",
       "      <td>0.0079428</td>\n",
       "      <td>0.123802</td>\n",
       "      <td>0.017844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.034264</td>\n",
       "      <td>-0.0335805</td>\n",
       "      <td>-0.034116</td>\n",
       "      <td>-0.0262708</td>\n",
       "      <td>-0.041478</td>\n",
       "      <td>-0.030769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.024646</td>\n",
       "      <td>0.0243655</td>\n",
       "      <td>0.024407</td>\n",
       "      <td>0.0110838</td>\n",
       "      <td>0.015390</td>\n",
       "      <td>0.017979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.033708</td>\n",
       "      <td>-0.0363252</td>\n",
       "      <td>-0.033766</td>\n",
       "      <td>-0.0347171</td>\n",
       "      <td>-0.032961</td>\n",
       "      <td>-0.034008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.022271</td>\n",
       "      <td>-0.0215996</td>\n",
       "      <td>-0.022205</td>\n",
       "      <td>-0.0162287</td>\n",
       "      <td>-0.023201</td>\n",
       "      <td>-0.022951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.013788</td>\n",
       "      <td></td>\n",
       "      <td>-0.013852</td>\n",
       "      <td>-0.0115437</td>\n",
       "      <td>-0.017520</td>\n",
       "      <td>-0.013504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.027737</td>\n",
       "      <td>0.0291922</td>\n",
       "      <td>0.027675</td>\n",
       "      <td>0.021478</td>\n",
       "      <td>0.018292</td>\n",
       "      <td>0.026802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.005272</td>\n",
       "      <td></td>\n",
       "      <td>-0.005389</td>\n",
       "      <td>-0.00413452</td>\n",
       "      <td>0.004344</td>\n",
       "      <td>-0.006843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.055191</td>\n",
       "      <td>-0.0607112</td>\n",
       "      <td>-0.055099</td>\n",
       "      <td>-0.0574818</td>\n",
       "      <td>-0.041382</td>\n",
       "      <td>-0.054279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.026904</td>\n",
       "      <td>-0.0293885</td>\n",
       "      <td>-0.026895</td>\n",
       "      <td>-0.0234961</td>\n",
       "      <td>-0.025664</td>\n",
       "      <td>-0.026155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.039642</td>\n",
       "      <td>-0.0396358</td>\n",
       "      <td>-0.039586</td>\n",
       "      <td>-0.0361107</td>\n",
       "      <td>-0.033411</td>\n",
       "      <td>-0.040528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.070167</td>\n",
       "      <td>-0.0714928</td>\n",
       "      <td>-0.070013</td>\n",
       "      <td>-0.0658111</td>\n",
       "      <td>-0.071342</td>\n",
       "      <td>-0.072111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.073018</td>\n",
       "      <td>-0.0741199</td>\n",
       "      <td>-0.072845</td>\n",
       "      <td>-0.0687759</td>\n",
       "      <td>-0.077932</td>\n",
       "      <td>-0.073230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.030958</td>\n",
       "      <td>-0.0308962</td>\n",
       "      <td>-0.030889</td>\n",
       "      <td>-0.0256791</td>\n",
       "      <td>-0.037046</td>\n",
       "      <td>-0.030900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.034508</td>\n",
       "      <td>-0.0347039</td>\n",
       "      <td>-0.034465</td>\n",
       "      <td>-0.0305059</td>\n",
       "      <td>-0.032529</td>\n",
       "      <td>-0.033574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-0.070402</td>\n",
       "      <td>-0.0709607</td>\n",
       "      <td>-0.070088</td>\n",
       "      <td>-0.0634438</td>\n",
       "      <td>-0.073105</td>\n",
       "      <td>-0.071526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-0.045213</td>\n",
       "      <td>-0.047722</td>\n",
       "      <td>-0.045045</td>\n",
       "      <td>-0.0346071</td>\n",
       "      <td>-0.047048</td>\n",
       "      <td>-0.044315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-0.010640</td>\n",
       "      <td></td>\n",
       "      <td>-0.010635</td>\n",
       "      <td>-0.00699729</td>\n",
       "      <td>-0.002788</td>\n",
       "      <td>-0.010475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.141574</td>\n",
       "      <td>0.141693</td>\n",
       "      <td>0.141283</td>\n",
       "      <td>0.140215</td>\n",
       "      <td>0.138186</td>\n",
       "      <td>0.141690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.108642</td>\n",
       "      <td>0.109051</td>\n",
       "      <td>0.108518</td>\n",
       "      <td>0.107465</td>\n",
       "      <td>0.101822</td>\n",
       "      <td>0.110653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.020717</td>\n",
       "      <td>0.0206109</td>\n",
       "      <td>0.020751</td>\n",
       "      <td>0.0163932</td>\n",
       "      <td>0.016755</td>\n",
       "      <td>0.019224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.007711</td>\n",
       "      <td></td>\n",
       "      <td>0.007764</td>\n",
       "      <td>0.00657535</td>\n",
       "      <td>-0.033586</td>\n",
       "      <td>0.007979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.031019</td>\n",
       "      <td>0.037562</td>\n",
       "      <td>0.031016</td>\n",
       "      <td>0.0244123</td>\n",
       "      <td>0.049922</td>\n",
       "      <td>0.027107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.099296</td>\n",
       "      <td>0.0990452</td>\n",
       "      <td>0.099071</td>\n",
       "      <td>0.0995843</td>\n",
       "      <td>0.096118</td>\n",
       "      <td>0.100664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Error</th>\n",
       "      <td>0.440321</td>\n",
       "      <td>0.440922</td>\n",
       "      <td>0.440323</td>\n",
       "      <td>0.441459</td>\n",
       "      <td>0.444069</td>\n",
       "      <td>0.440428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Error</th>\n",
       "      <td>0.466466</td>\n",
       "      <td>0.469403</td>\n",
       "      <td>0.466316</td>\n",
       "      <td>0.467408</td>\n",
       "      <td>0.470212</td>\n",
       "      <td>0.466749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   LS Best Subset     Ridge       Lasso       PCR       PLS\n",
       "0           -0.036110   -0.036637 -0.035866  -0.0267818 -0.033798 -0.036663\n",
       "1           -0.030507  -0.0298764 -0.030408  -0.0234702 -0.046519 -0.030949\n",
       "2            0.035183   0.0360065  0.035209   0.0323751  0.071214  0.035090\n",
       "3            0.038179   0.0380054  0.038124   0.0341091  0.028679  0.038801\n",
       "4            0.110689    0.111304  0.110466    0.107658  0.106853  0.110986\n",
       "5            0.060405   0.0606352  0.060368    0.058904  0.061114  0.060327\n",
       "6            0.175143    0.175378  0.174792    0.175833  0.163806  0.175843\n",
       "7            0.081245   0.0819069  0.081119   0.0798151  0.087373  0.081557\n",
       "8            0.059772   0.0623436  0.059723     0.05934  0.062577  0.059179\n",
       "9            0.011132              0.011173   0.0084412  0.013562  0.009840\n",
       "10           0.024762   0.0259669  0.024868   0.0221559  0.022915  0.023782\n",
       "11          -0.050242  -0.0506995 -0.050070  -0.0443741 -0.051794 -0.048921\n",
       "12           0.001425              0.001521              0.003185  0.000387\n",
       "13           0.001205              0.001259             -0.000210  0.002607\n",
       "14           0.009176              0.009375  0.00878644 -0.000502  0.012162\n",
       "15           0.111747     0.11146  0.111604    0.110345  0.114369  0.112386\n",
       "16           0.040973   0.0411687  0.041118   0.0404225  0.040790  0.040286\n",
       "17           0.059603   0.0615841  0.059549   0.0568506  0.062264  0.058067\n",
       "18           0.059005   0.0598366  0.059048   0.0590946  0.054173  0.060147\n",
       "19           0.063041   0.0626461  0.062978   0.0614178  0.061859  0.063540\n",
       "20           0.127755    0.127696  0.127561    0.129239  0.126516  0.126854\n",
       "21           0.099860    0.100121  0.099527   0.0948315  0.100711  0.096833\n",
       "22           0.120000    0.123441  0.119767    0.119998  0.127623  0.117878\n",
       "23           0.077301    0.077647  0.077177   0.0737166  0.072786  0.078084\n",
       "24          -0.077107   -0.077094 -0.076984  -0.0746125 -0.072865 -0.077810\n",
       "25          -0.041225  -0.0410929 -0.041257  -0.0398726 -0.046861 -0.041525\n",
       "26          -0.084050  -0.0839872 -0.083867  -0.0778929 -0.081057 -0.086010\n",
       "27           0.005401              0.005218             -0.003230  0.001284\n",
       "28          -0.009712             -0.009750 -0.00015639 -0.020460 -0.009257\n",
       "29          -0.045091   -0.044008 -0.044876  -0.0328331 -0.039689 -0.040435\n",
       "30          -0.020500  -0.0210813 -0.020315 -0.00295302 -0.016801 -0.014108\n",
       "31           0.026319              0.023337  0.00746307 -0.057982  0.017925\n",
       "32          -0.042810  -0.0452533 -0.042741  -0.0395651 -0.039637 -0.043188\n",
       "33           0.011649   0.0350203  0.014424   0.0079428  0.123802  0.017844\n",
       "34          -0.034264  -0.0335805 -0.034116  -0.0262708 -0.041478 -0.030769\n",
       "35           0.024646   0.0243655  0.024407   0.0110838  0.015390  0.017979\n",
       "36          -0.033708  -0.0363252 -0.033766  -0.0347171 -0.032961 -0.034008\n",
       "37          -0.022271  -0.0215996 -0.022205  -0.0162287 -0.023201 -0.022951\n",
       "38          -0.013788             -0.013852  -0.0115437 -0.017520 -0.013504\n",
       "39           0.027737   0.0291922  0.027675    0.021478  0.018292  0.026802\n",
       "40          -0.005272             -0.005389 -0.00413452  0.004344 -0.006843\n",
       "41          -0.055191  -0.0607112 -0.055099  -0.0574818 -0.041382 -0.054279\n",
       "42          -0.026904  -0.0293885 -0.026895  -0.0234961 -0.025664 -0.026155\n",
       "43          -0.039642  -0.0396358 -0.039586  -0.0361107 -0.033411 -0.040528\n",
       "44          -0.070167  -0.0714928 -0.070013  -0.0658111 -0.071342 -0.072111\n",
       "45          -0.073018  -0.0741199 -0.072845  -0.0687759 -0.077932 -0.073230\n",
       "46          -0.030958  -0.0308962 -0.030889  -0.0256791 -0.037046 -0.030900\n",
       "47          -0.034508  -0.0347039 -0.034465  -0.0305059 -0.032529 -0.033574\n",
       "48          -0.070402  -0.0709607 -0.070088  -0.0634438 -0.073105 -0.071526\n",
       "49          -0.045213   -0.047722 -0.045045  -0.0346071 -0.047048 -0.044315\n",
       "50          -0.010640             -0.010635 -0.00699729 -0.002788 -0.010475\n",
       "51           0.141574    0.141693  0.141283    0.140215  0.138186  0.141690\n",
       "52           0.108642    0.109051  0.108518    0.107465  0.101822  0.110653\n",
       "53           0.020717   0.0206109  0.020751   0.0163932  0.016755  0.019224\n",
       "54           0.007711              0.007764  0.00657535 -0.033586  0.007979\n",
       "55           0.031019    0.037562  0.031016   0.0244123  0.049922  0.027107\n",
       "56           0.099296   0.0990452  0.099071   0.0995843  0.096118  0.100664\n",
       "Train Error  0.440321    0.440922  0.440323    0.441459  0.444069  0.440428\n",
       "Test Error   0.466466    0.469403  0.466316    0.467408  0.470212  0.466749"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a table matching books for spam data\n",
    "display_table(loaded_train_X, loaded_train_Y, loaded_test_X, loaded_test_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
